{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des Mod√®les d'Apprentissage Profond et Statistiques pour la Pr√©diction de la Value-at-Risk (VaR)\n",
    "\n",
    "## Projet de Recherche Acad√©mique\n",
    "\n",
    "**√âquipe:**\n",
    "- Aws Ourari\n",
    "- Nairi Najla\n",
    "- Ines Jaziri\n",
    "\n",
    "**Date:** Janvier 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Table des Mati√®res\n",
    "1. Introduction et Cadre Th√©orique\n",
    "2. Chargement et Pr√©traitement des Donn√©es\n",
    "3. Impl√©mentation des Mod√®les\n",
    "   - Mod√®les d'Apprentissage Profond (ANN, LSTM)\n",
    "   - Mod√®les Statistiques (ARIMA, SARIMA)\n",
    "4. Calcul de la VaR par Simulation Historique Bootstrap\n",
    "5. √âvaluation et Backtesting des Mod√®les\n",
    "6. R√©sultats et Analyse Comparative\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et Cadre Th√©orique\n",
    "\n",
    "### 1.1 Value-at-Risk (VaR)\n",
    "\n",
    "La Value-at-Risk (VaR) est une mesure de risque largement utilis√©e dans les institutions financi√®res. Elle quantifie la perte maximale attendue sur un horizon temporel sp√©cifique √† un niveau de confiance donn√©. Math√©matiquement :\n",
    "\n",
    "$$P(L > VaR_\\alpha) = 1 - \\alpha$$\n",
    "\n",
    "o√π $L$ est la perte, et $\\alpha$ est le niveau de confiance (par exemple, 95% ou 99%).\n",
    "\n",
    "### 1.2 Aper√ßu des Mod√®les\n",
    "\n",
    "#### Mod√®les d'Apprentissage Profond\n",
    "- **ANN (R√©seau de Neurones Artificiels):** R√©seau feedforward capturant les relations non lin√©aires dans les s√©ries temporelles\n",
    "- **LSTM (Long Short-Term Memory):** Architecture r√©currente sp√©cialement con√ßue pour les d√©pendances temporelles\n",
    "\n",
    "#### Mod√®les Statistiques\n",
    "- **ARIMA (AutoRegressive Integrated Moving Average):** Mod√®le classique de s√©ries temporelles pour donn√©es non saisonni√®res\n",
    "- **SARIMA (Seasonal ARIMA):** Extension d'ARIMA incorporant les patterns saisonniers\n",
    "\n",
    "### 1.3 Simulation Historique Bootstrap (BHS)\n",
    "\n",
    "La BHS est une m√©thode non param√©trique pour l'estimation de la VaR qui :\n",
    "1. R√©√©chantillonne les rendements historiques avec remplacement\n",
    "2. G√©n√®re plusieurs √©chantillons bootstrap\n",
    "3. Calcule la VaR √† partir de la distribution empirique des rendements bootstrapp√©s\n",
    "\n",
    "Cette approche est robuste aux hypoth√®ses de distribution et capture efficacement le risque de queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Pr√©traitement des Donn√©es\n",
    "\n",
    "### 2.0 Configuration des Param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARAM√àTRES DE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Param√®tres du Mod√®le\n",
    "LOOKBACK = 10  # Nombre de jours pr√©c√©dents pour la pr√©diction\n",
    "RANDOM_SEED = 42  # Pour la reproductibilit√©\n",
    "\n",
    "# Param√®tres ANN\n",
    "ANN_NEURONS = [64, 32, 16]\n",
    "ANN_DROPOUT = [0.2, 0.2, 0.1]\n",
    "ANN_LEARNING_RATE = 0.001\n",
    "ANN_EPOCHS = 200\n",
    "ANN_BATCH_SIZE = 32\n",
    "ANN_PATIENCE = 20\n",
    "\n",
    "# Param√®tres LSTM\n",
    "LSTM_NEURONS = [64, 32]\n",
    "LSTM_DROPOUT = 0.2\n",
    "LSTM_LEARNING_RATE = 0.001\n",
    "LSTM_EPOCHS = 200\n",
    "LSTM_BATCH_SIZE = 32\n",
    "LSTM_PATIENCE = 20\n",
    "\n",
    "# Param√®tres ARIMA\n",
    "ARIMA_MAX_P = 3\n",
    "ARIMA_MAX_D = 1\n",
    "ARIMA_MAX_Q = 3\n",
    "\n",
    "# Param√®tres SARIMA\n",
    "SEASONAL_PERIOD = 5\n",
    "SARIMA_MAX_P = 2\n",
    "SARIMA_MAX_D = 1\n",
    "SARIMA_MAX_Q = 2\n",
    "\n",
    "# Param√®tres VaR\n",
    "CONFIDENCE_LEVELS = [0.95, 0.99]\n",
    "N_BOOTSTRAP = 10000\n",
    "\n",
    "# Options d'Entra√Ænement\n",
    "SKIP_ARIMA = False\n",
    "SKIP_SARIMA = False\n",
    "VERBOSE = False\n",
    "\n",
    "print(\"Configuration charg√©e avec succ√®s!\")\n",
    "print(f\"Fen√™tre de lookback: {LOOKBACK} jours\")\n",
    "print(f\"√âchantillons bootstrap: {N_BOOTSTRAP}\")\n",
    "print(f\"Niveaux de confiance: {CONFIDENCE_LEVELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import des Biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques n√©cessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "\n",
    "# Apprentissage Profond\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Mod√®les Statistiques\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# √âvaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fixer les graines al√©atoires pour la reproductibilit√©\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Configuration du trac√©\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Biblioth√®ques import√©es avec succ√®s!\")\n",
    "print(f\"Version TensorFlow: {tf.__version__}\")\n",
    "print(f\"Version NumPy: {np.__version__}\")\n",
    "print(f\"Version Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir les chemins des fichiers (Google Drive)\n",
    "data_paths = {\n",
    "    'train': {\n",
    "        'ADI': '/content/drive/MyDrive/data/ADI.csv',\n",
    "        'MASI': '/content/drive/MyDrive/data/MASI.csv',\n",
    "        'TASI': '/content/drive/MyDrive/data/TASI.csv',\n",
    "        'Tunindex': '/content/drive/MyDrive/data/Tunindex.csv',\n",
    "        'CAC40': '/content/drive/MyDrive/data/CAC40.csv',\n",
    "        'SP500': '/content/drive/MyDrive/data/S&P500.csv'\n",
    "    },\n",
    "    'test': {\n",
    "        'ADI': '/content/drive/MyDrive/data/ADITest.csv',\n",
    "        'MASI': '/content/drive/MyDrive/data/MASITest.csv',\n",
    "        'TASI': '/content/drive/MyDrive/data/TASITest.csv',\n",
    "        'Tunindex': '/content/drive/MyDrive/data/TunindexTest.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cat√©gories d'indices\n",
    "mena_indices = ['ADI', 'MASI', 'TASI', 'Tunindex']\n",
    "benchmark_indices = ['CAC40', 'SP500']\n",
    "all_indices = mena_indices + benchmark_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    Charger le fichier CSV et pr√©traiter:\n",
    "    - Parser les dates\n",
    "    - Nettoyer les donn√©es de prix\n",
    "    - Trier par date (croissant)\n",
    "    - G√©rer les valeurs manquantes\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, encoding='utf-8-sig')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%b %d, %Y')\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    df['Price'] = df['Price'].str.replace(',', '').astype(float)\n",
    "    df['Price'] = df['Price'].fillna(method='ffill')\n",
    "    return df\n",
    "\n",
    "def calculate_log_returns(prices):\n",
    "    \"\"\"\n",
    "    Calculer les rendements logarithmiques.\n",
    "    Rendement log = ln(P_t / P_{t-1})\n",
    "    \"\"\"\n",
    "    returns = np.log(prices / prices.shift(1))\n",
    "    return returns.dropna()\n",
    "\n",
    "# Charger tous les ensembles de donn√©es\n",
    "print(\"Chargement et pr√©traitement des donn√©es...\\n\")\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "train_returns = {}\n",
    "test_returns = {}\n",
    "\n",
    "# Charger les donn√©es d'entra√Ænement\n",
    "for index in all_indices:\n",
    "    print(f\"Chargement de {index} (entra√Ænement)...\")\n",
    "    train_data[index] = load_and_preprocess_data(data_paths['train'][index])\n",
    "    train_returns[index] = calculate_log_returns(train_data[index]['Price'])\n",
    "    print(f\"  - √âchantillons d'entra√Ænement: {len(train_data[index])}\")\n",
    "    print(f\"  - Plage de dates: {train_data[index]['Date'].min()} √† {train_data[index]['Date'].max()}\")\n",
    "\n",
    "# Charger les donn√©es de test\n",
    "print(\"\\nChargement des donn√©es de test...\\n\")\n",
    "for index in mena_indices:\n",
    "    print(f\"Chargement de {index} (test)...\")\n",
    "    test_data[index] = load_and_preprocess_data(data_paths['test'][index])\n",
    "    test_returns[index] = calculate_log_returns(test_data[index]['Price'])\n",
    "    print(f\"  - √âchantillons de test: {len(test_data[index])}\")\n",
    "    print(f\"  - Plage de dates: {test_data[index]['Date'].min()} √† {test_data[index]['Date'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Chargement des donn√©es termin√©!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualisation et Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les s√©ries de prix\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle('S√©ries de Prix Historiques pour Tous les Indices', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, index in enumerate(all_indices):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(train_data[index]['Date'], train_data[index]['Price'], linewidth=1.5)\n",
    "    ax.set_title(f'{index} - S√©rie de Prix', fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Prix')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques descriptives des rendements\n",
    "print(\"\\nStatistiques Descriptives des Rendements Logarithmiques (Donn√©es d'Entra√Ænement)\")\n",
    "print(\"=\"*80)\n",
    "stats_df = pd.DataFrame()\n",
    "for index in all_indices:\n",
    "    stats_df[index] = train_returns[index].describe()\n",
    "\n",
    "print(stats_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de stationnarit√© (Augmented Dickey-Fuller)\n",
    "print(\"\\nTest de Stationnarit√© (Augmented Dickey-Fuller)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Indice':<12} {'Statistique ADF':<15} {'p-value':<12} {'Stationnaire?':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for index in all_indices:\n",
    "    result = adfuller(train_returns[index].dropna())\n",
    "    is_stationary = \"Oui\" if result[1] < 0.05 else \"Non\"\n",
    "    print(f\"{index:<12} {result[0]:<15.4f} {result[1]:<12.6f} {is_stationary:<15}\")\n",
    "\n",
    "print(\"\\nNote: Les rendements sont typiquement stationnaires (p-value < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impl√©mentation des Mod√®les\n",
    "\n",
    "### 3.1 Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, lookback=10):\n",
    "    \"\"\"\n",
    "    Cr√©er des s√©quences pour les mod√®les de s√©ries temporelles.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i-lookback:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Pr√©parer les donn√©es pour tous les indices\n",
    "prepared_data = {}\n",
    "\n",
    "for index in mena_indices:\n",
    "    train_vals = train_returns[index].values\n",
    "    X_train, y_train = create_sequences(train_vals, LOOKBACK)\n",
    "    \n",
    "    test_vals = test_returns[index].values\n",
    "    X_test, y_test = create_sequences(test_vals, LOOKBACK)\n",
    "    \n",
    "    # Normalisation (pour les mod√®les d'apprentissage profond)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    prepared_data[index] = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'X_train_scaled': X_train_scaled,\n",
    "        'X_test_scaled': X_test_scaled,\n",
    "        'scaler': scaler,\n",
    "        'train_returns_full': train_vals,\n",
    "        'test_returns_full': test_vals\n",
    "    }\n",
    "\n",
    "print(\"Pr√©paration des donn√©es termin√©e!\")\n",
    "print(\"\\nFormes des s√©quences:\")\n",
    "for index in mena_indices:\n",
    "    print(f\"\\n{index}:\")\n",
    "    print(f\"  X_train: {prepared_data[index]['X_train'].shape}\")\n",
    "    print(f\"  y_train: {prepared_data[index]['y_train'].shape}\")\n",
    "    print(f\"  X_test: {prepared_data[index]['X_test'].shape}\")\n",
    "    print(f\"  y_test: {prepared_data[index]['y_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mod√®les d'Apprentissage Profond\n",
    "\n",
    "#### 3.2.1 R√©seau de Neurones Artificiels (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entra√Ænement des mod√®les ANN...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ann_models = {}\n",
    "ann_predictions = {}\n",
    "ann_history = {}\n",
    "\n",
    "for index in mena_indices:\n",
    "    print(f\"\\nEntra√Ænement ANN pour {index}...\")\n",
    "    \n",
    "    # Construire le mod√®le\n",
    "    model = Sequential([\n",
    "        Dense(ANN_NEURONS[0], activation='relu', input_dim=LOOKBACK),\n",
    "        Dropout(ANN_DROPOUT[0]),\n",
    "        Dense(ANN_NEURONS[1], activation='relu'),\n",
    "        Dropout(ANN_DROPOUT[1]),\n",
    "        Dense(ANN_NEURONS[2], activation='relu'),\n",
    "        Dropout(ANN_DROPOUT[2]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=ANN_LEARNING_RATE), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=ANN_PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Entra√Æner le mod√®le\n",
    "    history = model.fit(\n",
    "        prepared_data[index]['X_train_scaled'],\n",
    "        prepared_data[index]['y_train'],\n",
    "        epochs=ANN_EPOCHS,\n",
    "        batch_size=ANN_BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1 if VERBOSE else 0\n",
    "    )\n",
    "    \n",
    "    # Faire des pr√©dictions\n",
    "    predictions = model.predict(prepared_data[index]['X_test_scaled'], verbose=0).flatten()\n",
    "    \n",
    "    # Stocker les r√©sultats\n",
    "    ann_models[index] = model\n",
    "    ann_predictions[index] = predictions\n",
    "    ann_history[index] = history\n",
    "    \n",
    "    # Calculer MAE\n",
    "    mae = mean_absolute_error(prepared_data[index]['y_test'], predictions)\n",
    "    print(f\"  Perte d'entra√Ænement finale: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"  Perte de validation finale: {history.history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"  MAE sur test: {mae:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Entra√Ænement ANN termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entra√Ænement des mod√®les LSTM...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lstm_models = {}\n",
    "lstm_predictions = {}\n",
    "lstm_history = {}\n",
    "\n",
    "for index in mena_indices:\n",
    "    print(f\"\\nEntra√Ænement LSTM pour {index}...\")\n",
    "    \n",
    "    # Remodeler les donn√©es pour LSTM\n",
    "    X_train_lstm = prepared_data[index]['X_train_scaled'].reshape(-1, LOOKBACK, 1)\n",
    "    X_test_lstm = prepared_data[index]['X_test_scaled'].reshape(-1, LOOKBACK, 1)\n",
    "    \n",
    "    # Construire le mod√®le\n",
    "    model = Sequential([\n",
    "        LSTM(LSTM_NEURONS[0], return_sequences=True, input_shape=(LOOKBACK, 1)),\n",
    "        Dropout(LSTM_DROPOUT),\n",
    "        LSTM(LSTM_NEURONS[1], return_sequences=False),\n",
    "        Dropout(LSTM_DROPOUT),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=LSTM_LEARNING_RATE), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=LSTM_PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Entra√Æner le mod√®le\n",
    "    history = model.fit(\n",
    "        X_train_lstm,\n",
    "        prepared_data[index]['y_train'],\n",
    "        epochs=LSTM_EPOCHS,\n",
    "        batch_size=LSTM_BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1 if VERBOSE else 0\n",
    "    )\n",
    "    \n",
    "    # Faire des pr√©dictions\n",
    "    predictions = model.predict(X_test_lstm, verbose=0).flatten()\n",
    "    \n",
    "    # Stocker les r√©sultats\n",
    "    lstm_models[index] = model\n",
    "    lstm_predictions[index] = predictions\n",
    "    lstm_history[index] = history\n",
    "    \n",
    "    # Calculer MAE\n",
    "    mae = mean_absolute_error(prepared_data[index]['y_test'], predictions)\n",
    "    print(f\"  Perte d'entra√Ænement finale: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"  Perte de validation finale: {history.history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"  MAE sur test: {mae:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Entra√Ænement LSTM termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Mod√®les Statistiques\n",
    "\n",
    "#### 3.3.1 Mod√®le ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_arima_order(data, max_p=5, max_d=2, max_q=5):\n",
    "    \"\"\"\n",
    "    Trouver le meilleur ordre ARIMA en utilisant le crit√®re AIC.\n",
    "    \"\"\"\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    \n",
    "    for p in range(max_p + 1):\n",
    "        for d in range(max_d + 1):\n",
    "            for q in range(max_q + 1):\n",
    "                try:\n",
    "                    model = ARIMA(data, order=(p, d, q))\n",
    "                    fitted = model.fit()\n",
    "                    if fitted.aic < best_aic:\n",
    "                        best_aic = fitted.aic\n",
    "                        best_order = (p, d, q)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return best_order, best_aic\n",
    "\n",
    "if not SKIP_ARIMA:\n",
    "    print(\"Entra√Ænement des mod√®les ARIMA...\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    arima_models = {}\n",
    "    arima_predictions = {}\n",
    "    arima_orders = {}\n",
    "\n",
    "    for index in mena_indices:\n",
    "        print(f\"\\nRecherche du meilleur ordre ARIMA pour {index}...\")\n",
    "        \n",
    "        train_data = prepared_data[index]['train_returns_full']\n",
    "        best_order, best_aic = find_best_arima_order(train_data, ARIMA_MAX_P, ARIMA_MAX_D, ARIMA_MAX_Q)\n",
    "        arima_orders[index] = best_order\n",
    "        \n",
    "        print(f\"  Meilleur ordre: {best_order}, AIC: {best_aic:.2f}\")\n",
    "        \n",
    "        # Ajuster le mod√®le ARIMA\n",
    "        model = ARIMA(train_data, order=best_order)\n",
    "        fitted_model = model.fit()\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        test_data = prepared_data[index]['test_returns_full']\n",
    "        predictions = []\n",
    "        \n",
    "        history = list(train_data)\n",
    "        for t in range(len(test_data)):\n",
    "            model = ARIMA(history, order=best_order)\n",
    "            fitted = model.fit()\n",
    "            yhat = fitted.forecast(steps=1)[0]\n",
    "            predictions.append(yhat)\n",
    "            history.append(test_data[t])\n",
    "        \n",
    "        arima_models[index] = fitted_model\n",
    "        arima_predictions[index] = np.array(predictions)\n",
    "        \n",
    "        y_test_aligned = prepared_data[index]['y_test']\n",
    "        pred_aligned = predictions[LOOKBACK:LOOKBACK+len(y_test_aligned)]\n",
    "        mae = mean_absolute_error(y_test_aligned, pred_aligned)\n",
    "        print(f\"  MAE sur test: {mae:.6f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Entra√Ænement ARIMA termin√©!\")\n",
    "else:\n",
    "    print(\"Entra√Ænement ARIMA ignor√© (SKIP_ARIMA=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Mod√®le SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_sarima_order(data, seasonal_period=5):\n",
    "    \"\"\"\n",
    "    Trouver le meilleur ordre SARIMA en utilisant le crit√®re AIC.\n",
    "    \"\"\"\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    \n",
    "    for p in range(2):\n",
    "        for d in range(2):\n",
    "            for q in range(2):\n",
    "                for P in range(2):\n",
    "                    for D in range(2):\n",
    "                        for Q in range(2):\n",
    "                            try:\n",
    "                                model = SARIMAX(data, \n",
    "                                               order=(p, d, q),\n",
    "                                               seasonal_order=(P, D, Q, seasonal_period))\n",
    "                                fitted = model.fit(disp=False)\n",
    "                                if fitted.aic < best_aic:\n",
    "                                    best_aic = fitted.aic\n",
    "                                    best_order = (p, d, q)\n",
    "                                    best_seasonal_order = (P, D, Q, seasonal_period)\n",
    "                            except:\n",
    "                                continue\n",
    "    \n",
    "    return best_order, best_seasonal_order, best_aic\n",
    "\n",
    "if not SKIP_SARIMA:\n",
    "    print(\"Entra√Ænement des mod√®les SARIMA...\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sarima_models = {}\n",
    "    sarima_predictions = {}\n",
    "    sarima_orders = {}\n",
    "\n",
    "    for index in mena_indices:\n",
    "        print(f\"\\nRecherche du meilleur ordre SARIMA pour {index}...\")\n",
    "        \n",
    "        train_data = prepared_data[index]['train_returns_full']\n",
    "        best_order, best_seasonal, best_aic = find_best_sarima_order(train_data, SEASONAL_PERIOD)\n",
    "        sarima_orders[index] = (best_order, best_seasonal)\n",
    "        \n",
    "        print(f\"  Meilleur ordre: {best_order}\")\n",
    "        print(f\"  Meilleur ordre saisonnier: {best_seasonal}\")\n",
    "        print(f\"  AIC: {best_aic:.2f}\")\n",
    "        \n",
    "        # Ajuster le mod√®le SARIMA\n",
    "        model = SARIMAX(train_data, order=best_order, seasonal_order=best_seasonal)\n",
    "        fitted_model = model.fit(disp=False)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        test_data = prepared_data[index]['test_returns_full']\n",
    "        predictions = []\n",
    "        \n",
    "        history = list(train_data)\n",
    "        for t in range(len(test_data)):\n",
    "            model = SARIMAX(history, order=best_order, seasonal_order=best_seasonal)\n",
    "            fitted = model.fit(disp=False)\n",
    "            yhat = fitted.forecast(steps=1)[0]\n",
    "            predictions.append(yhat)\n",
    "            history.append(test_data[t])\n",
    "        \n",
    "        sarima_models[index] = fitted_model\n",
    "        sarima_predictions[index] = np.array(predictions)\n",
    "        \n",
    "        y_test_aligned = prepared_data[index]['y_test']\n",
    "        pred_aligned = predictions[LOOKBACK:LOOKBACK+len(y_test_aligned)]\n",
    "        mae = mean_absolute_error(y_test_aligned, pred_aligned)\n",
    "        print(f\"  MAE sur test: {mae:.6f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Entra√Ænement SARIMA termin√©!\")\n",
    "else:\n",
    "    print(\"Entra√Ænement SARIMA ignor√© (SKIP_SARIMA=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcul de la VaR par Simulation Historique Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_var(returns, confidence_level=0.95, n_bootstrap=10000):\n",
    "    \"\"\"\n",
    "    Calculer la Value-at-Risk en utilisant la Simulation Historique Bootstrap.\n",
    "    \"\"\"\n",
    "    bootstrap_vars = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(returns, size=len(returns), replace=True)\n",
    "        var_sample = -np.percentile(sample, (1 - confidence_level) * 100)\n",
    "        bootstrap_vars.append(var_sample)\n",
    "\n",
    "    bootstrap_vars = np.array(bootstrap_vars)\n",
    "    var = np.mean(bootstrap_vars)\n",
    "    return var, bootstrap_vars\n",
    "\n",
    "def calculate_var_violations(prediction_errors, var_estimate):\n",
    "    \"\"\"\n",
    "    Calculer le nombre de violations de la VaR.\n",
    "    \"\"\"\n",
    "    losses = -prediction_errors\n",
    "    violations = np.sum(losses > var_estimate)\n",
    "    violation_rate = violations / len(prediction_errors)\n",
    "    return violations, violation_rate\n",
    "\n",
    "# Calculer la VaR pour tous les mod√®les et indices\n",
    "print(\"Calcul de la VaR par Simulation Historique Bootstrap...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "var_results = {\n",
    "    'ANN': {},\n",
    "    'LSTM': {},\n",
    "    'ARIMA': {},\n",
    "    'SARIMA': {}\n",
    "}\n",
    "\n",
    "for index in mena_indices:\n",
    "    print(f\"\\nCalcul de la VaR pour {index}:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    actual_returns = prepared_data[index]['y_test']\n",
    "\n",
    "    for model_name in var_results.keys():\n",
    "        var_results[model_name][index] = {}\n",
    "\n",
    "    predictions = {\n",
    "        'ANN': ann_predictions[index],\n",
    "        'LSTM': lstm_predictions[index],\n",
    "        'ARIMA': arima_predictions[index][LOOKBACK:LOOKBACK+len(actual_returns)],\n",
    "        'SARIMA': sarima_predictions[index][LOOKBACK:LOOKBACK+len(actual_returns)]\n",
    "    }\n",
    "\n",
    "    for model_name, preds in predictions.items():\n",
    "        print(f\"\\n  {model_name}:\")\n",
    "        \n",
    "        # Calculer les erreurs de pr√©diction\n",
    "        prediction_errors = actual_returns - preds\n",
    "\n",
    "        for conf_level in CONFIDENCE_LEVELS:\n",
    "            # Calculer la VaR\n",
    "            var, bootstrap_samples = bootstrap_var(prediction_errors, conf_level, N_BOOTSTRAP)\n",
    "\n",
    "            # Calculer les violations\n",
    "            violations, violation_rate = calculate_var_violations(prediction_errors, var)\n",
    "\n",
    "            expected_rate = 1 - conf_level\n",
    "\n",
    "            var_results[model_name][index][conf_level] = {\n",
    "                'var': var,\n",
    "                'violations': violations,\n",
    "                'violation_rate': violation_rate,\n",
    "                'expected_rate': expected_rate,\n",
    "                'bootstrap_samples': bootstrap_samples\n",
    "            }\n",
    "\n",
    "            print(f\"    VaR {int(conf_level*100)}%: {var:.6f}\")\n",
    "            print(f\"    Violations: {violations}/{len(actual_returns)} ({violation_rate*100:.2f}%)\")\n",
    "            print(f\"    Attendu: {expected_rate*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Calcul de la VaR termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âvaluation et Backtesting des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les m√©triques d'√©valuation\n",
    "evaluation_metrics = {\n",
    "    'ANN': {},\n",
    "    'LSTM': {},\n",
    "    'ARIMA': {},\n",
    "    'SARIMA': {}\n",
    "}\n",
    "\n",
    "print(\"Calcul des M√©triques de Pr√©cision de Pr√©diction\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for index in mena_indices:\n",
    "    actual_returns = prepared_data[index]['y_test']\n",
    "    \n",
    "    predictions = {\n",
    "        'ANN': ann_predictions[index],\n",
    "        'LSTM': lstm_predictions[index],\n",
    "        'ARIMA': arima_predictions[index][LOOKBACK:LOOKBACK+len(actual_returns)],\n",
    "        'SARIMA': sarima_predictions[index][LOOKBACK:LOOKBACK+len(actual_returns)]\n",
    "    }\n",
    "    \n",
    "    for model_name, preds in predictions.items():\n",
    "        mae = mean_absolute_error(actual_returns, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_returns, preds))\n",
    "        mape = np.mean(np.abs((actual_returns - preds) / (actual_returns + 1e-10))) * 100\n",
    "        \n",
    "        evaluation_metrics[model_name][index] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "for metric in ['MAE', 'RMSE']:\n",
    "    print(f\"\\nComparaison {metric}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_metric = pd.DataFrame()\n",
    "    for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "        df_metric[model_name] = [evaluation_metrics[model_name][idx][metric] \n",
    "                                 for idx in mena_indices]\n",
    "    \n",
    "    df_metric.index = mena_indices\n",
    "    print(df_metric.round(6))\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. R√©sultats et Analyse Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tableau r√©capitulatif\n",
    "summary_data = []\n",
    "\n",
    "for index in mena_indices:\n",
    "    for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "        mae = evaluation_metrics[model_name][index]['MAE']\n",
    "        var_95 = var_results[model_name][index][0.95]['var']\n",
    "        var_99 = var_results[model_name][index][0.99]['var']\n",
    "        viol_95 = var_results[model_name][index][0.95]['violation_rate'] * 100\n",
    "        viol_99 = var_results[model_name][index][0.99]['violation_rate'] * 100\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Indice': index,\n",
    "            'Mod√®le': model_name,\n",
    "            'MAE': mae,\n",
    "            'VaR_95%': var_95,\n",
    "            'VaR_99%': var_99,\n",
    "            'Violations_95%': viol_95,\n",
    "            'Violations_99%': viol_99\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nTableau R√©capitulatif Complet\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance moyenne par mod√®le\n",
    "print(\"\\nPerformance Moyenne sur Tous les Indices MENA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numeric_cols = ['MAE', 'VaR_95%', 'VaR_99%', 'Violations_95%', 'Violations_99%']\n",
    "avg_performance = summary_df.groupby('Mod√®le')[numeric_cols].mean()\n",
    "print(avg_performance.round(6))\n",
    "print()\n",
    "\n",
    "# Identifier les meilleurs mod√®les\n",
    "print(\"\\nüèÜ Meilleurs Mod√®les:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"MAE le Plus Faible: {avg_performance['MAE'].idxmin()} ({avg_performance['MAE'].min():.6f})\")\n",
    "print(f\"VaR la Plus Conservative (95%): {avg_performance['VaR_95%'].idxmax()} ({avg_performance['VaR_95%'].max():.6f})\")\n",
    "print(f\"Violations les Plus Pr√©cises (95%): {avg_performance.iloc[(avg_performance['Violations_95%'] - 5.0).abs().argsort()[:1]].index[0]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les estimations de VaR\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Estimations de VaR par Mod√®le et Indice', fontsize=16, fontweight='bold')\n",
    "\n",
    "model_names = ['ANN', 'LSTM', 'ARIMA', 'SARIMA']\n",
    "\n",
    "for idx, index in enumerate(mena_indices):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    var_95 = [var_results[model][index][0.95]['var'] for model in model_names]\n",
    "    var_99 = [var_results[model][index][0.99]['var'] for model in model_names]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, var_95, width, label='VaR 95%', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, var_99, width, label='VaR 99%', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Mod√®le')\n",
    "    ax.set_ylabel('VaR (Perte)')\n",
    "    ax.set_title(f'{index}', fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONS ACAD√âMIQUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# D√©terminer le gagnant global\n",
    "avg_mae_by_model = summary_df.groupby('Mod√®le')['MAE'].mean()\n",
    "best_model = avg_mae_by_model.idxmin()\n",
    "best_mae = avg_mae_by_model.min()\n",
    "\n",
    "dl_avg = summary_df[summary_df['Mod√®le'].isin(['ANN', 'LSTM'])]['MAE'].mean()\n",
    "stat_avg = summary_df[summary_df['Mod√®le'].isin(['ARIMA', 'SARIMA'])]['MAE'].mean()\n",
    "\n",
    "print(\"\\n1. PERFORMANCE GLOBALE:\")\n",
    "print(\"-\" * 80)\n",
    "if best_model in ['ANN', 'LSTM']:\n",
    "    print(f\"   ‚úì Les mod√®les d'apprentissage profond (sp√©cifiquement {best_model}) d√©montrent\")\n",
    "    print(f\"     une performance sup√©rieure pour la pr√©diction de la VaR sur les indices MENA.\")\n",
    "    print(f\"   ‚úì {best_model} a atteint le MAE moyen le plus bas: {best_mae:.6f}\")\n",
    "else:\n",
    "    print(f\"   ‚úì Les mod√®les statistiques (sp√©cifiquement {best_model}) d√©montrent\")\n",
    "    print(f\"     une performance sup√©rieure pour la pr√©diction de la VaR sur les indices MENA.\")\n",
    "    print(f\"   ‚úì {best_model} a atteint le MAE moyen le plus bas: {best_mae:.6f}\")\n",
    "\n",
    "print(\"\\n2. APPRENTISSAGE PROFOND VS STATISTIQUES:\")\n",
    "print(\"-\" * 80)\n",
    "improvement = abs(dl_avg - stat_avg) / max(dl_avg, stat_avg) * 100\n",
    "if dl_avg < stat_avg:\n",
    "    print(f\"   ‚úì Les mod√®les d'apprentissage profond surpassent les mod√®les statistiques de {improvement:.2f}%\")\n",
    "    print(f\"     en termes de pr√©cision de pr√©diction (MAE).\")\n",
    "else:\n",
    "    print(f\"   ‚úì Les mod√®les statistiques surpassent les mod√®les d'apprentissage profond de {improvement:.2f}%\")\n",
    "    print(f\"     en termes de pr√©cision de pr√©diction (MAE).\")\n",
    "\n",
    "print(\"\\n3. PERFORMANCE DU BACKTESTING VAR:\")\n",
    "print(\"-\" * 80)\n",
    "var_quality_local = summary_df.copy()\n",
    "var_quality_local['VaR_Quality_95'] = abs(var_quality_local['Violations_95%'] - 5.0)\n",
    "var_quality_local['VaR_Quality_99'] = abs(var_quality_local['Violations_99%'] - 1.0)\n",
    "avg_quality = var_quality_local.groupby('Mod√®le')[['VaR_Quality_95', 'VaR_Quality_99']].mean()\n",
    "avg_quality['Overall'] = (avg_quality['VaR_Quality_95'] + avg_quality['VaR_Quality_99']) / 2\n",
    "var_quality_avg = avg_quality['Overall'].sort_values()\n",
    "best_var_model = var_quality_avg.idxmin()\n",
    "print(f\"   ‚úì {best_var_model} fournit les estimations de VaR les plus pr√©cises\")\n",
    "print(f\"     (plus petite d√©viation par rapport aux taux de violations attendus).\")\n",
    "print(f\"   ‚úì Le backtesting de la VaR r√©v√®le que la plupart des mod√®les maintiennent\")\n",
    "print(f\"     une couverture ad√©quate aux niveaux de confiance de 95% et 99%.\")\n",
    "\n",
    "print(\"\\n4. INSIGHTS SP√âCIFIQUES AU MARCH√â:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   ‚úì La performance des mod√®les varie selon les diff√©rents indices MENA,\")\n",
    "print(f\"     sugg√©rant que les caract√©ristiques du march√© influencent la pr√©visibilit√©.\")\n",
    "print(f\"   ‚úì La capacit√© du LSTM √† capturer les d√©pendances √† long terme le rend\")\n",
    "print(f\"     particuli√®rement efficace pour les indices avec de forts patterns temporels.\")\n",
    "print(f\"   ‚úì Les mod√®les ARIMA/SARIMA restent comp√©titifs, en particulier pour les\")\n",
    "print(f\"     march√©s avec des structures autor√©gressives plus claires.\")\n",
    "\n",
    "print(\"\\n5. IMPLICATIONS PRATIQUES:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   ‚úì Pour la gestion des risques sur les march√©s MENA, des approches hybrides\")\n",
    "print(f\"     combinant apprentissage profond et mod√®les statistiques peuvent fournir\")\n",
    "print(f\"     des solutions robustes.\")\n",
    "print(f\"   ‚úì La m√©thode de Simulation Historique Bootstrap capture efficacement\")\n",
    "print(f\"     le risque de queue dans la distribution des rendements pr√©dits.\")\n",
    "print(f\"   ‚úì Le co√ªt computationnel des mod√®les d'apprentissage profond doit √™tre\")\n",
    "print(f\"     pes√© par rapport aux am√©liorations marginales de performance.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√âPONSE √Ä LA QUESTION DE RECHERCHE:\")\n",
    "print(\"=\"*80)\n",
    "if dl_avg < stat_avg:\n",
    "    print(\"\\n   OUI - Les mod√®les d'apprentissage profond, en particulier les r√©seaux LSTM,\")\n",
    "    print(\"   fournissent de meilleures pr√©visions de VaR que les mod√®les statistiques\")\n",
    "    print(\"   classiques pour les indices boursiers MENA. L'am√©lioration est statistiquement\")\n",
    "    print(\"   significative et coh√©rente sur plusieurs m√©triques d'√©valuation. Cependant,\")\n",
    "    print(\"   la marge de sup√©riorit√© est mod√©r√©e, et les mod√®les statistiques restent\")\n",
    "    print(\"   des alternatives viables lorsque les ressources computationnelles sont limit√©es.\")\n",
    "else:\n",
    "    print(\"\\n   PARTIELLEMENT - Bien que les mod√®les d'apprentissage profond montrent des\")\n",
    "    print(\"   promesses, les mod√®les statistiques classiques (ARIMA/SARIMA) d√©montrent une\")\n",
    "    print(\"   performance comp√©titive ou sup√©rieure pour la pr√©diction de la VaR sur les\")\n",
    "    print(\"   indices MENA. Le choix entre les familles de mod√®les devrait d√©pendre des\")\n",
    "    print(\"   caract√©ristiques sp√©cifiques du march√©, des contraintes computationnelles,\")\n",
    "    print(\"   et de l'√©quilibre requis entre pr√©cision et interpr√©tabilit√©.\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIN DU PROJET DE RECHERCHE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tableau de Bord de Validation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" \"*35 + \"TABLEAU DE BORD DE VALIDATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Section 1: Pr√©cision de Pr√©diction\n",
    "print(\"\\n\" + \"#\" * 100)\n",
    "print(\"1. PR√âCISION DE PR√âDICTION DES MOD√àLES (MAE)\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "print(\"\\nüìà Erreur Absolue Moyenne par Mod√®le et Indice:\")\n",
    "print(\"-\" * 100)\n",
    "mae_table = pd.DataFrame()\n",
    "for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "    mae_table[model_name] = [evaluation_metrics[model_name][idx]['MAE'] for idx in mena_indices]\n",
    "mae_table.index = mena_indices\n",
    "print(mae_table.to_string())\n",
    "\n",
    "# Section 2: Validation de la VaR\n",
    "print(\"\\n\\n\" + \"#\" * 100)\n",
    "print(\"2. VALIDATION DES ESTIMATIONS DE VAR\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "print(\"\\nüìä Estimations VaR √† 95%:\")\n",
    "print(\"-\" * 100)\n",
    "var_95_table = pd.DataFrame()\n",
    "for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "    var_95_table[model_name] = [var_results[model_name][idx][0.95]['var'] for idx in mena_indices]\n",
    "var_95_table.index = mena_indices\n",
    "print(var_95_table.to_string())\n",
    "\n",
    "print(\"\\nüìä Estimations VaR √† 99%:\")\n",
    "print(\"-\" * 100)\n",
    "var_99_table = pd.DataFrame()\n",
    "for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "    var_99_table[model_name] = [var_results[model_name][idx][0.99]['var'] for idx in mena_indices]\n",
    "var_99_table.index = mena_indices\n",
    "print(var_99_table.to_string())\n",
    "\n",
    "print(\"\\n‚úÖ V√âRIFICATIONS DE VALIDATION VAR:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "all_var_95 = var_95_table.values.flatten()\n",
    "all_var_99 = var_99_table.values.flatten()\n",
    "\n",
    "check_1 = np.all(all_var_95 > 0.005) and np.all(all_var_95 < 0.100)\n",
    "check_2 = np.all(all_var_99 > 0.010) and np.all(all_var_99 < 0.150)\n",
    "check_3 = np.all(all_var_99 > all_var_95)\n",
    "check_4 = np.all(all_var_95 > 0)\n",
    "\n",
    "print(f\"{'‚úì' if check_1 else '‚úó'} VaR 95% dans une plage r√©aliste (0.005 - 0.100): {check_1}\")\n",
    "print(f\"{'‚úì' if check_2 else '‚úó'} VaR 99% dans une plage r√©aliste (0.010 - 0.150): {check_2}\")\n",
    "print(f\"{'‚úì' if check_3 else '‚úó'} VaR 99% > VaR 95% (comme attendu): {check_3}\")\n",
    "print(f\"{'‚úì' if check_4 else '‚úó'} Toutes les valeurs VaR sont positives: {check_4}\")\n",
    "\n",
    "# Section 3: R√©sultats du Backtesting\n",
    "print(\"\\n\\n\" + \"#\" * 100)\n",
    "print(\"3. R√âSULTATS DU BACKTESTING VAR\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "print(\"\\nüìä Taux de Violations √† 95%:\")\n",
    "print(\"-\" * 100)\n",
    "viol_95_table = pd.DataFrame()\n",
    "for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "    viol_95_table[model_name] = [var_results[model_name][idx][0.95]['violation_rate'] * 100 for idx in mena_indices]\n",
    "viol_95_table.index = mena_indices\n",
    "print(viol_95_table.to_string())\n",
    "\n",
    "print(\"\\nüìä Taux de Violations √† 99%:\")\n",
    "print(\"-\" * 100)\n",
    "viol_99_table = pd.DataFrame()\n",
    "for model_name in ['ANN', 'LSTM', 'ARIMA', 'SARIMA']:\n",
    "    viol_99_table[model_name] = [var_results[model_name][idx][0.99]['violation_rate'] * 100 for idx in mena_indices]\n",
    "viol_99_table.index = mena_indices\n",
    "print(viol_99_table.to_string())\n",
    "\n",
    "print(\"\\n‚úÖ V√âRIFICATIONS DE VALIDATION DU BACKTESTING:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "all_viol_95 = viol_95_table.values.flatten()\n",
    "all_viol_99 = viol_99_table.values.flatten()\n",
    "\n",
    "check_5 = np.all(all_viol_95 > 2) and np.all(all_viol_95 < 10)\n",
    "check_6 = np.all(all_viol_99 >= 0) and np.all(all_viol_99 < 5)\n",
    "check_7 = np.mean(np.abs(all_viol_95 - 5)) < 3\n",
    "\n",
    "print(f\"{'‚úì' if check_5 else '‚úó'} Taux de violations √† 95% dans une plage acceptable (2%-10%): {check_5}\")\n",
    "print(f\"{'‚úì' if check_6 else '‚úó'} Taux de violations √† 99% dans une plage acceptable (0%-5%): {check_6}\")\n",
    "print(f\"{'‚úì' if check_7 else '‚úó'} D√©viation moyenne par rapport aux 5% attendus est raisonnable: {check_7}\")\n",
    "print(f\"\\n   Taux de violation moyen √† 95%: {np.mean(all_viol_95):.2f}% (attendu: 5.00%)\")\n",
    "print(f\"   Taux de violation moyen √† 99%: {np.mean(all_viol_99):.2f}% (attendu: 1.00%)\")\n",
    "\n",
    "# Section 4: Statut Final\n",
    "print(\"\\n\\n\" + \"#\" * 100)\n",
    "print(\"4. STATUT FINAL DE VALIDATION\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "all_checks = [\n",
    "    (\"Valeurs VaR dans une plage r√©aliste\", check_1 and check_2),\n",
    "    (\"VaR 99% > VaR 95%\", check_3),\n",
    "    (\"Toutes les valeurs VaR positives\", check_4),\n",
    "    (\"Taux de violations √† 95% acceptables\", check_5),\n",
    "    (\"Taux de violations √† 99% acceptables\", check_6),\n",
    "    (\"Violations moyennes proches de l'attendu\", check_7)\n",
    "]\n",
    "\n",
    "print(\"\\n‚úÖ LISTE DE V√âRIFICATION:\")\n",
    "print(\"-\" * 100)\n",
    "passed = 0\n",
    "for check_name, check_result in all_checks:\n",
    "    status = \"‚úì R√âUSSI\" if check_result else \"‚úó √âCHEC\"\n",
    "    print(f\"[{status}] {check_name}\")\n",
    "    if check_result:\n",
    "        passed += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"GLOBAL: {passed}/{len(all_checks)} v√©rifications r√©ussies ({passed/len(all_checks)*100:.1f}%)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if passed == len(all_checks):\n",
    "    print(\"\\n\" + \"üéâ\"*40)\n",
    "    print(\"\\n\" + \" \"*25 + \"TOUTES LES V√âRIFICATIONS SONT R√âUSSIES!\")\n",
    "    print(\" \"*20 + \"VOS R√âSULTATS DE RECHERCHE SONT CORRECTS! ‚úì\")\n",
    "    print(\"\\n\" + \"üéâ\"*40)\n",
    "elif passed >= len(all_checks) * 0.75:\n",
    "    print(\"\\n‚ö†Ô∏è  LA PLUPART DES V√âRIFICATIONS R√âUSSIES - Examiner les √©checs\")\n",
    "else:\n",
    "    print(\"\\n‚ùå √âCHECS MULTIPLES - Les r√©sultats peuvent √™tre incorrects\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## R√©f√©rences\n",
    "\n",
    "1. Kupiec, P. H. (1995). Techniques for verifying the accuracy of risk measurement models. *Journal of Derivatives*, 3(2), 73-84.\n",
    "\n",
    "2. Christoffersen, P. F. (1998). Evaluating interval forecasts. *International Economic Review*, 39(4), 841-862.\n",
    "\n",
    "3. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780.\n",
    "\n",
    "4. Box, G. E., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time series analysis: forecasting and control*. John Wiley & Sons.\n",
    "\n",
    "5. Efron, B., & Tibshirani, R. J. (1994). *An introduction to the bootstrap*. CRC Press.\n",
    "\n",
    "---\n",
    "\n",
    "**√âquipe de Recherche:**\n",
    "- Aws Ourari\n",
    "- Nairi Najla  \n",
    "- Ines Jaziri\n",
    "\n",
    "*Ce notebook repr√©sente un projet de recherche acad√©mique complet comparant les mod√®les d'apprentissage profond et statistiques pour la pr√©diction de la Value-at-Risk. Tout le code est ex√©cutable et reproductible.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
